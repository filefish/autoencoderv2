{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "import os\nimport numpy as np\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nimport pylab\nimport torch.optim as optim\nfrom torch.nn import functional as F"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [],
      "source": "num_epochs \u003d 100\nbatch_size \u003d 128\nseed \u003d 1\nout_dir \u003d \u0027./vae_20\u0027",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "cuda is available!\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\ncuda \u003d torch.cuda.is_available()\nif cuda:\n    print(\u0027cuda is available!\u0027)\n\nif not os.path.exists(out_dir):\n    os.mkdir(out_dir)\n\ntorch.manual_seed(seed)\nif cuda:\n    torch.cuda.manual_seed(seed)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "outputs": [],
      "source": "train_loader \u003d torch.utils.data.DataLoader(\n    datasets.MNIST(\u0027../data\u0027,\n                   train\u003dTrue,\n                   download\u003dTrue,\n                   transform\u003dtransforms.ToTensor()),\n    batch_size\u003dbatch_size,\n    shuffle\u003dTrue\n)\n\ntest_loader \u003d torch.utils.data.DataLoader(\n    datasets.MNIST(\u0027../data\u0027,\n                   train\u003dFalse,\n                   transform\u003dtransforms.ToTensor()),\n    batch_size\u003dbatch_size,\n    shuffle\u003dTrue\n)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "\u003ctorch.utils.data.dataloader.DataLoader object at 0x7f0570bdac88\u003e\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(train_loader)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "outputs": [],
      "source": "\nclass VAE(nn.Module):\n\n    def __init__(self):\n        super(VAE, self).__init__()\n        \n        self.fc1 \u003d nn.Linear(784, 512)\n        self.fc21 \u003d nn.Linear(512, 20)  # mu\n        self.fc22 \u003d nn.Linear(512, 20)  # logvar\n\n        self.fc3 \u003d nn.Linear(20, 512)\n        self.fc4 \u003d nn.Linear(512, 784)\n        \n        self.relu \u003d nn.ReLU()\n        self.sigmoid \u003d nn.Sigmoid()\n    \n    def encode(self, x):\n        h \u003d self.relu(self.fc1(x))\n        return self.fc21(h), self.fc22(h)\n\n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std \u003d logvar.mul(0.5).exp_()\n            eps \u003d Variable(std.data.new(std.size()).normal_())\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n    def decode(self, z):\n        h \u003d self.relu(self.fc3(z))\n        return self.sigmoid(self.fc4(h))\n    \n    def forward(self, x):\n        x \u003d x.view(-1, 784)\n        mu, logvar \u003d self.encode(x)\n        z \u003d self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\nmodel \u003d VAE()\nif cuda:\n    model.cuda()\noptimizer \u003d optim.Adam(model.parameters(), lr\u003d1e-3)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "outputs": [],
      "source": "\ndef loss_function(recon_x, x, mu, logvar):\n    # size_average\u003dFalseなのでバッチ内のサンプルの合計lossを求める\n    # reconstruction loss 入力画像をどのくらい正確に復元できたか？\n    # 数式では対数尤度の最大化だが交差エントロピーlossの最小化と等価\n    recon \u003d F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average\u003dFalse)\n\n    # 潜在空間zに対する正則化項\n    # P(z|x) が N(0, I)に近くなる（KL-distanceが小さくなる）ようにする\n    kld \u003d -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    return recon + kld",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "outputs": [],
      "source": "def train(epoch):\n    model.train()\n    train_loss \u003d 0\n    for batch_idx, (data, _) in enumerate(train_loader):\n        if cuda:\n            data \u003d Variable(data.cuda())\n        else:\n            data \u003d Variable(data)\n        optimizer.zero_grad()\n        recon_batch, mu, logvar \u003d model(data)\n        loss \u003d loss_function(recon_batch, data, mu, logvar)\n        loss.backward()\n        train_loss +\u003d loss.item()\n        optimizer.step()\n    \n    # loss_function() は平均ではなく全サンプルの合計lossを返すのでサンプル数で割る\n    train_loss /\u003d len(train_loader.dataset)\n\n    return train_loss    \n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "outputs": [],
      "source": "\ndef test(epoch):\n    model.eval()\n    test_loss \u003d 0\n    for batch_idx, (data, _) in enumerate(test_loader):\n        if cuda:\n            data \u003d Variable(data.cuda(), volatile\u003dTrue)\n        else:\n            data \u003d Variable(data, volatile\u003dTrue)\n        recon_batch, mu, logvar \u003d model(data)\n        loss \u003d loss_function(recon_batch, data, mu, logvar)\n        test_loss +\u003d loss.item()\n        \n        if epoch % 10 \u003d\u003d 0:\n            # 10エポックごとに最初のminibatchの入力画像と復元画像を保存\n            if batch_idx \u003d\u003d 0:\n                n \u003d 8\n                comparison \u003d torch.cat([data[:n],\n                                        recon_batch.view(batch_size, 1, 28, 28)[:n]])\n                save_image(comparison.data.cpu(),\n                           \u0027{}/reconstruction_{}_20.png\u0027.format(out_dir, epoch), nrow\u003dn)\n\n    test_loss /\u003d len(test_loader.dataset)\n\n    return test_loss",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "/home/user/PycharmProjects/auto_encoder/venv/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n  import sys\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "epoch [2/100], loss: 158.7105 test_loss: 115.2169\n",
            "epoch [3/100], loss: 118.4853 test_loss: 104.9736\n",
            "epoch [4/100], loss: 112.7418 test_loss: 101.1061\n",
            "epoch [5/100], loss: 110.1834 test_loss: 100.1671\n",
            "epoch [6/100], loss: 108.6221 test_loss: 99.0382\n",
            "epoch [7/100], loss: 107.5751 test_loss: 98.0739\n",
            "epoch [8/100], loss: 106.8707 test_loss: 97.2110\n",
            "epoch [9/100], loss: 106.2958 test_loss: 96.7292\n",
            "epoch [10/100], loss: 105.8637 test_loss: 96.1738\n",
            "epoch [11/100], loss: 105.4890 test_loss: 95.9200\n",
            "epoch [12/100], loss: 105.1402 test_loss: 96.2292\n",
            "epoch [13/100], loss: 104.8197 test_loss: 95.6445\n",
            "epoch [14/100], loss: 104.6068 test_loss: 95.8513\n",
            "epoch [15/100], loss: 104.4030 test_loss: 95.6735\n",
            "epoch [16/100], loss: 104.1800 test_loss: 94.9153\n",
            "epoch [17/100], loss: 104.0063 test_loss: 95.3870\n",
            "epoch [18/100], loss: 103.8504 test_loss: 95.4805\n",
            "epoch [19/100], loss: 103.6045 test_loss: 95.2434\n",
            "epoch [20/100], loss: 103.5206 test_loss: 95.3382\n",
            "epoch [21/100], loss: 103.3319 test_loss: 95.2700\n",
            "epoch [22/100], loss: 103.2372 test_loss: 94.7774\n",
            "epoch [23/100], loss: 103.1290 test_loss: 95.2411\n",
            "epoch [24/100], loss: 102.9664 test_loss: 94.1958\n",
            "epoch [25/100], loss: 102.8848 test_loss: 94.2986\n",
            "epoch [26/100], loss: 102.7504 test_loss: 94.0007\n",
            "epoch [27/100], loss: 102.6536 test_loss: 94.3482\n",
            "epoch [28/100], loss: 102.5778 test_loss: 94.2555\n",
            "epoch [29/100], loss: 102.4416 test_loss: 93.7173\n",
            "epoch [30/100], loss: 102.3679 test_loss: 94.2010\n",
            "epoch [31/100], loss: 102.2917 test_loss: 93.9021\n",
            "epoch [32/100], loss: 102.2144 test_loss: 94.2717\n",
            "epoch [33/100], loss: 102.1428 test_loss: 93.9731\n",
            "epoch [34/100], loss: 102.0703 test_loss: 93.4849\n",
            "epoch [35/100], loss: 101.9455 test_loss: 93.4828\n",
            "epoch [36/100], loss: 101.9042 test_loss: 93.4663\n",
            "epoch [37/100], loss: 101.8371 test_loss: 93.8484\n",
            "epoch [38/100], loss: 101.7859 test_loss: 93.9835\n",
            "epoch [39/100], loss: 101.7175 test_loss: 93.5879\n",
            "epoch [40/100], loss: 101.6529 test_loss: 93.6514\n",
            "epoch [41/100], loss: 101.5834 test_loss: 93.9144\n",
            "epoch [42/100], loss: 101.4927 test_loss: 93.4611\n",
            "epoch [43/100], loss: 101.4685 test_loss: 92.9817\n",
            "epoch [44/100], loss: 101.3967 test_loss: 93.4089\n",
            "epoch [45/100], loss: 101.3979 test_loss: 93.9122\n",
            "epoch [46/100], loss: 101.2702 test_loss: 92.6711\n",
            "epoch [47/100], loss: 101.2218 test_loss: 93.3053\n",
            "epoch [48/100], loss: 101.2114 test_loss: 93.1372\n",
            "epoch [49/100], loss: 101.1506 test_loss: 93.3906\n",
            "epoch [50/100], loss: 101.1144 test_loss: 93.8512\n",
            "epoch [51/100], loss: 101.0712 test_loss: 92.9002\n",
            "epoch [52/100], loss: 101.0098 test_loss: 92.6517\n",
            "epoch [53/100], loss: 100.9708 test_loss: 92.6819\n",
            "epoch [54/100], loss: 100.9116 test_loss: 93.0941\n",
            "epoch [55/100], loss: 100.9104 test_loss: 93.0976\n",
            "epoch [56/100], loss: 100.8684 test_loss: 92.3225\n",
            "epoch [57/100], loss: 100.8036 test_loss: 92.7896\n",
            "epoch [58/100], loss: 100.7715 test_loss: 93.1412\n",
            "epoch [59/100], loss: 100.7463 test_loss: 92.7445\n",
            "epoch [60/100], loss: 100.7526 test_loss: 93.0617\n",
            "epoch [61/100], loss: 100.6382 test_loss: 92.5326\n",
            "epoch [62/100], loss: 100.6390 test_loss: 92.8569\n",
            "epoch [63/100], loss: 100.5890 test_loss: 92.6523\n",
            "epoch [64/100], loss: 100.6053 test_loss: 92.5028\n",
            "epoch [65/100], loss: 100.5020 test_loss: 92.4746\n",
            "epoch [66/100], loss: 100.5170 test_loss: 93.0806\n",
            "epoch [67/100], loss: 100.5322 test_loss: 92.8619\n",
            "epoch [68/100], loss: 100.4720 test_loss: 93.0036\n",
            "epoch [69/100], loss: 100.4434 test_loss: 93.1044\n",
            "epoch [70/100], loss: 100.4496 test_loss: 93.0602\n",
            "epoch [71/100], loss: 100.3616 test_loss: 92.6359\n",
            "epoch [72/100], loss: 100.3570 test_loss: 91.9090\n",
            "epoch [73/100], loss: 100.3365 test_loss: 92.5163\n",
            "epoch [74/100], loss: 100.3066 test_loss: 92.2403\n",
            "epoch [75/100], loss: 100.2886 test_loss: 92.5212\n",
            "epoch [76/100], loss: 100.2589 test_loss: 92.6745\n",
            "epoch [77/100], loss: 100.2230 test_loss: 92.7920\n",
            "epoch [78/100], loss: 100.2245 test_loss: 92.6459\n",
            "epoch [79/100], loss: 100.1870 test_loss: 92.8477\n",
            "epoch [80/100], loss: 100.1512 test_loss: 92.0090\n",
            "epoch [81/100], loss: 100.1653 test_loss: 92.4037\n",
            "epoch [82/100], loss: 100.1051 test_loss: 92.4070\n",
            "epoch [83/100], loss: 100.0869 test_loss: 92.6984\n",
            "epoch [84/100], loss: 100.0341 test_loss: 92.2133\n",
            "epoch [85/100], loss: 100.0926 test_loss: 92.5113\n",
            "epoch [86/100], loss: 100.0299 test_loss: 92.3124\n",
            "epoch [87/100], loss: 100.0492 test_loss: 92.7512\n",
            "epoch [88/100], loss: 100.0317 test_loss: 93.2098\n",
            "epoch [89/100], loss: 99.9796 test_loss: 92.4746\n",
            "epoch [90/100], loss: 99.9257 test_loss: 92.5621\n",
            "epoch [91/100], loss: 99.9206 test_loss: 92.4087\n",
            "epoch [92/100], loss: 99.8935 test_loss: 92.5798\n",
            "epoch [93/100], loss: 99.8761 test_loss: 91.5382\n",
            "epoch [94/100], loss: 99.9111 test_loss: 92.3298\n",
            "epoch [95/100], loss: 99.8323 test_loss: 92.6061\n",
            "epoch [96/100], loss: 99.8379 test_loss: 92.3211\n",
            "epoch [97/100], loss: 99.8373 test_loss: 92.2028\n",
            "epoch [98/100], loss: 99.8443 test_loss: 92.0294\n",
            "epoch [99/100], loss: 99.8028 test_loss: 92.2870\n",
            "epoch [100/100], loss: 99.7844 test_loss: 91.9773\n",
            "epoch [101/100], loss: 99.7483 test_loss: 92.3738\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "loss_list \u003d []\ntest_loss_list \u003d []\nfor epoch in range(1, num_epochs + 1):\n    loss \u003d train(epoch)\n    test_loss \u003d test(epoch)\n\n    print(\u0027epoch [{}/{}], loss: {:.4f} test_loss: {:.4f}\u0027.format(\n        epoch + 1,\n        num_epochs,\n        loss,\n        test_loss))\n\n    # logging\n    loss_list.append(loss)\n    test_loss_list.append(test_loss)\n\n# save the training model\nnp.save(\u0027./vae_20/loss_list_20.npy\u0027, np.array(loss_list))\nnp.save(\u0027./vae_20/test_loss_list_20.npy\u0027, np.array(test_loss_list))\ntorch.save(model.state_dict(), \u0027./vae_20/vae_20.pth\u0027)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-5-e14259f9ee41\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027matplotlib\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\u0027inline\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m \u001b[0mloss_list\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027{}/loss_list_20.npy\u0027\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027epoch\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name \u0027np\u0027 is not defined"
          ],
          "ename": "NameError",
          "evalue": "name \u0027np\u0027 is not defined",
          "output_type": "error"
        }
      ],
      "source": "\nimport matplotlib.pyplot as plt\n%matplotlib inline\nloss_list \u003d np.load(\u0027{}/loss_list_20.npy\u0027.format(out_dir))\nplt.plot(loss_list)\nplt.xlabel(\u0027epoch\u0027)\nplt.ylabel(\u0027loss\u0027)\nplt.grid()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-6-6f73f4ed9375\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m \u001b[0mtest_loss_list\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027{}/test_loss_list_20.npy\u0027\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027epoch\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027test loss\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name \u0027np\u0027 is not defined"
          ],
          "ename": "NameError",
          "evalue": "name \u0027np\u0027 is not defined",
          "output_type": "error"
        }
      ],
      "source": "test_loss_list \u003d np.load(\u0027{}/test_loss_list_20.npy\u0027.format(out_dir))\nplt.plot(test_loss_list)\nplt.xlabel(\u0027epoch\u0027)\nplt.ylabel(\u0027test loss\u0027)\nplt.grid()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-7-f8f878c8453b\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[0;32m----\u003e 1\u001b[0;31m model.load_state_dict(torch.load(\u0027{}/vae.pth\u0027.format(out_dir),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                  \u001b[0mmap_location\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  loc: storage))\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027./data\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name \u0027model\u0027 is not defined"
          ],
          "ename": "NameError",
          "evalue": "name \u0027model\u0027 is not defined",
          "output_type": "error"
        }
      ],
      "source": "model.load_state_dict(torch.load(\u0027{}/vae.pth\u0027.format(out_dir),\n                                 map_location\u003dlambda storage,\n                                 loc: storage))\ntest_dataset \u003d datasets.MNIST(\u0027./data\u0027, download\u003dTrue, train\u003dFalse, transform\u003dtransforms.ToTensor())\ntest_loader \u003d torch.utils.data.DataLoader(test_dataset, batch_size\u003d10000, shuffle\u003dFalse)\nimages, labels \u003d iter(test_loader).next()\nimages \u003d images.view(10000, -1)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "(10000, 2) (10000, 2)\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": [
            "/home/user/PycharmProjects/auto_encoder/venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n  \n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# 784次元ベクトルを2次元ベクトルにencode　ｚが２次元の場合をプロットしている。２０の場合はT-SNEなどを使って前処理する必要がありそう。\nz \u003d model.encode(Variable(images, volatile\u003dTrue).cuda())\nmu, logvar \u003d z\nmu, logvar \u003d mu.cpu().data.numpy(), logvar.cpu().data.numpy()\nprint(mu.shape, logvar.shape)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-4-360f8e324e80\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# plt.scatterは分布を図示する関数。makerには星や丸などの形が定義されており、c,cmapで色を定義している。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;34m\u0027.\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name \u0027plt\u0027 is not defined"
          ],
          "ename": "NameError",
          "evalue": "name \u0027plt\u0027 is not defined",
          "output_type": "error"
        }
      ],
      "source": "\n# plt.scatterは分布を図示する関数。makerには星や丸などの形が定義されており、c,cmapで色を定義している。\nplt.figure(figsize\u003d(10, 10))\nplt.scatter(mu[:, 0], mu[:, 1], marker\u003d\u0027.\u0027, c\u003dlabels.numpy(), cmap\u003dpylab.cm.jet)\nplt.colorbar()\nplt.xlim((-6, 6))\nplt.ylim((-6, 6))\nplt.grid()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}